{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization commands\n",
    "%matplotlib inline\n",
    "\n",
    "# Important libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import numpy.random as npr # random sampling functions\n",
    "import scipy.stats as sps # statistical functions\n",
    "import matplotlib.pyplot as plt # Matlab plotting framework\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.ticker as mticker\n",
    "import time\n",
    "\n",
    "# Important functions\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import array\n",
    "from scipy.io import loadmat\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "from matplotlib import colors as mcolors\n",
    "from taylorDiagram import TaylorDiagram\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "# Misc settings\n",
    "sns.set(context=\"poster\", style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basePath = os.getcwd()\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 50,\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-specifc Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix\n",
    "Cholesky decomposition of a Hermitian positive-definite matrix is defined as $\\Sigma = LDL^T$, where $L$ is any real-valued lower unit triangular matrix and $D$ is a diagonal matrix with positive elements. We allow the MVN proposal distribution to populate the elements of $L$ in such a way that all proposals provided by the jumping kernel are valid. The code below manipulates these raw MVN proposal locations to ensure the resulting $\\Sigma$ is always a Hermitian positive-definite matrix.\n",
    "\n",
    "$$\\theta^* \\sim\\ MVN(\\theta,\\lambda I)$$\n",
    "\n",
    "$$\\Sigma = LDL^T = \\left[ \\begin{array}{cccc}\n",
    "1 & 0 & 0 \\\\\n",
    "\\theta_{4} & 1 & 0 \\\\\n",
    "\\theta_{5} & \\theta_{6} & 1 \\\\ \\end{array} \\right]\n",
    "\\left[ \\begin{array}{cccc}\n",
    "\\exp(\\theta_{1}) & 0 & 0 \\\\\n",
    "0 & \\exp(\\theta_{2}) & 0 \\\\\n",
    "0 & 0 & \\exp(\\theta_{3}) \\\\ \\end{array} \\right]\n",
    "\\left[ \\begin{array}{cccc}\n",
    "1 & \\theta_{4} & \\theta_{5} \\\\\n",
    "0 & 1 & \\theta_{6} \\\\\n",
    "0 & 0 & 1 \\\\ \\end{array} \\right] =\n",
    "\\left[ \\begin{array}{cccc}\n",
    "\\sigma_{1}^2 & \\rho_{12}\\sigma_{1}\\sigma_{2} & \\rho_{13}\\sigma_{1}\\sigma_{3} \\\\\n",
    "\\rho_{12}\\sigma_{1}\\sigma_{2} & \\sigma_{2}^2 & \\rho_{23}\\sigma_{2}\\sigma_{3} \\\\\n",
    "\\rho_{13}\\sigma_{1}\\sigma_{3} & \\rho_{23}\\sigma_{2}\\sigma_{3} & \\sigma_{3}^2 \\\\ \\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tril2cov(x,m):\n",
    "    \n",
    "    D = np.diag(np.exp(x[:m]))\n",
    "    L = np.zeros((m,m))\n",
    "    L[np.tril_indices(m,-1)] = x[-(len(x)-m):]\n",
    "    L[np.diag_indices(m)] = np.ones(m) \n",
    "    \n",
    "    return np.dot(np.dot(L,D),L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking the Covariance Matrix After Sampling\n",
    "While variance parameter estimates can be useful, we desire inferences for the common elements of the correpsonding covariance matrix in our multivariate Bayesian model. Since the sampler operates on raw Cholesky decomposition parameters and then reshapes them into a covariance matrix with a helper function (i.e., tril2cov), the posterior samples we obtain aren't easily interpreted. As a result, we must convert Cholesky decomposition parameters into the inuitive components of the covariance matrix.\n",
    "\n",
    "$$\\Sigma = \\left[ \\begin{array}{cccc}\n",
    "\\sigma_{1}^2 & \\rho_{12}\\sigma_{1}\\sigma_{2} & \\rho_{13}\\sigma_{1}\\sigma_{3} \\\\\n",
    "\\rho_{12}\\sigma_{1}\\sigma_{2} & \\sigma_{2}^2 & \\rho_{23}\\sigma_{2}\\sigma_{3} \\\\\n",
    "\\rho_{13}\\sigma_{1}\\sigma_{3} & \\rho_{23}\\sigma_{2}\\sigma_{3} & \\sigma_{3}^2 \\\\ \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{cccc}\n",
    "1 & 0 & 0 \\\\\n",
    "\\theta_{4} & 1 & 0 \\\\\n",
    "\\theta_{5} & \\theta_{6} & 1 \\\\ \\end{array} \\right]\n",
    "\\left[ \\begin{array}{cccc}\n",
    "\\exp(\\theta_{1}) & 0 & 0 \\\\\n",
    "0 & \\exp(\\theta_{2}) & 0 \\\\\n",
    "0 & 0 & \\exp(\\theta_{3}) \\\\ \\end{array} \\right]\n",
    "\\left[ \\begin{array}{cccc}\n",
    "1 & \\theta_{4} & \\theta_{5} \\\\\n",
    "0 & 1 & \\theta_{6} \\\\\n",
    "0 & 0 & 1 \\\\ \\end{array} \\right] = LDL^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Elements: $\\rho_{ij}=\\frac{\\Sigma_{ij}}{\\sigma_{i}\\sigma_{j}}$, where $i = j \\implies \\rho_{ii}= \\rho_{jj}=1$\n",
    "We note that the parameters corresponding to the diagnoals of this matrix should be identically one. Only the Pearson product-moment correlation coefficients should survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov2cor(x,d):\n",
    "    \n",
    "    x = x.as_matrix()\n",
    "    n = x.shape[0] # number of samples\n",
    "    p = d*(d+1)/2 # total number of unique elements in the covariance matrix\n",
    "    m = d*(d-1)/2 # number of unique off-diagonal elements\n",
    "    \n",
    "    # preallocate arrays\n",
    "    sig = np.zeros((n,d))\n",
    "    pcc = np.zeros((n,m))\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        covM = tril2cov(x[i,-p:],d)\n",
    "        sig[i] = np.sqrt(np.diag(covM))  \n",
    "        covI = np.outer(sig[i],sig[i])\n",
    "        corM = covM/covI\n",
    "        pcc[i] = corM[np.tril_indices(d,-1)]\n",
    "    \n",
    "    return sig,pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization: Z-Scores\n",
    "The computational behavior of MLE and MCMC methods is improved when working with data has been shifted and scaled to have $\\mu=0$ and $\\sigma^2=1$ along each dimension of the data vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# center and/or scale data to have mean = 0 and variance = 1\n",
    "\n",
    "def zscore(data, mode=0):\n",
    "    \n",
    "    mu = np.mean(data)\n",
    "    sig = np.std(data)\n",
    "    \n",
    "    if mode == 0:\n",
    "        \n",
    "        z = (data - mu)/sig\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        z = data/sig\n",
    "    \n",
    "    return z,mu,sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide an inverse z-score transformation\n",
    "\n",
    "def zreturn(data, mu, sig, dim=0):\n",
    "    \n",
    "    return sig[dim]*data + mu[dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credible Estimates: The Highest Density Interval/Region\n",
    "Meaningful quantitative uncertainty statements are frequently delivered as interval estimates over the desired random variable(s). It is often convenient to idenitfy the narrowest such interval/region when summarizing the uncertainty associated with a statistical inference. The highest density interval is the narrowest interval that contains a specified probablistic mass fraction.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code modified for Python from Doing Bayesian Data Analysis: A Tutorial with R and BUGS by John K. Kruschke\n",
    "# Functions for approximating highest density intervals\n",
    "\n",
    "def HDI(data, massFrac=0.95):\n",
    "    \n",
    "    data = np.reshape(data,(1,-1))\n",
    "    sortData = np.sort(data).ravel()\n",
    "    ciIdxInc = np.floor(massFrac*len(sortData))\n",
    "    nCI = len(sortData) - ciIdxInc\n",
    "    \n",
    "    ciWidth = np.zeros((1,nCI)).ravel()\n",
    "    \n",
    "    for idx in range(int(nCI)):\n",
    "        \n",
    "        ciWidth[idx] = sortData[idx+ciIdxInc] - sortData[idx] \n",
    "    \n",
    "    return sortData[np.argmin(ciWidth)],sortData[np.argmin(ciWidth)+ciIdxInc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE Belief Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ykde(ax, xi):\n",
    "    \n",
    "    data_x, data_y = ax.lines[0].get_data()\n",
    "    \n",
    "    return np.interp(xi,data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freedman–Diaconis Rule: Histogram Parameter Determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal histogram bin parameters using the FD rule\n",
    "# mode 0 returns the optimal number of bins\n",
    "# mode 1 returns the optimal bin width\n",
    "\n",
    "def binFD(data, mode = 0):\n",
    "    \n",
    "    data = np.reshape(data,(1,-1))\n",
    "    data = np.sort(data).ravel()\n",
    "    bw = 2*sps.iqr(data)/(len(data)**(1/3))\n",
    "    \n",
    "    if mode == 0:\n",
    "        \n",
    "        funcAns = int(np.ptp(data)/bw)\n",
    "        \n",
    "    else:\n",
    "                    \n",
    "        funcAns = bw\n",
    "        \n",
    "    return funcAns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Function   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACF(data, lags):\n",
    "    \n",
    "    sac = np.zeros((lags))\n",
    "    arg = data - np.mean(data)\n",
    "    \n",
    "    for i in range(lags):\n",
    "        \n",
    "        sac[i] = np.inner(np.reshape(arg[-(data.size-(i+1)):],(1,-1)),\n",
    "                          np.reshape(arg[:(data.size-(i+1))],(1,-1)))\n",
    "    \n",
    "    return np.insert(sac/np.sum(arg**2),0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical CDF Helper Function   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \n",
    "    data = np.reshape(data,(1,-1))\n",
    "    sortData = np.sort(data).ravel()\n",
    "\n",
    "    return sortData, np.arange(1, len(sortData)+1)/float(len(sortData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-plot Tick Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_tick_formatter(val, pos=None):\n",
    "        \n",
    "    return \"{:.0e}\".format(10**val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diag(dx,burnVal,N,ix,basePath):\n",
    "    \n",
    "    plt.figure(figsize=(32,24))\n",
    "    \n",
    "    for j in range(nChains):\n",
    "        \n",
    "        pw = dx.Width[dx['Chain']==j]\n",
    "        plt.semilogx(np.arange(1,N+1), pw, label = r'$Chain$ ' + str(j), lw = 3)\n",
    "        \n",
    "    plt.xlabel('Proposal',  fontsize = 40)\n",
    "    plt.ylabel(r'Variance: log($\\sigma^2_k$)',  fontsize = 40, color='black')\n",
    "    plt.title('Adaptive Gaussian Kernel | Chain Length: ' + nStr + ' | ID: ' + mdl + ' ' + cityStr,  fontsize = 50)\n",
    "    xLim = plt.xlim()\n",
    "    yLim = plt.ylim()\n",
    "    plt.plot([burnVal,burnVal], [yLim[0], yLim[1]], color='r', label=r'$Burn$' + '-' + r'$in$', lw = 3)\n",
    "    plt.plot(xLim, [np.median(dx.Width),np.median(dx.Width)], label = '$\\mu_{1/2}$', color='black', lw = 3, ls='--')\n",
    "    plt.xlim(xmin=1,xmax=N)\n",
    "    plt.legend(prop={'size':25});\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/diag_pw.png');      \n",
    "    plt.clf()\n",
    "    \n",
    "    plt.figure(figsize=(32,24))\n",
    "    \n",
    "    for j in range(nChains):\n",
    "        \n",
    "        ns = dx.Accept[dx['Chain']==j]\n",
    "        plt.semilogx(np.arange(1,N+1), ns, label = r'$Chain$ ' + str(j), lw = 3)\n",
    "        \n",
    "    plt.xlabel('Proposal',  fontsize = 40)\n",
    "    plt.ylabel('Acceptance Rate',  fontsize = 40, color='black')\n",
    "    plt.title('Adaptive Gaussian Kernel | Chain Length: ' + nStr + ' | ID: ' + mdl + ' ' + cityStr,  fontsize = 50)\n",
    "    xLim = plt.xlim()\n",
    "    yLim = plt.ylim()\n",
    "    plt.plot([burnVal,burnVal], [yLim[0], yLim[1]], color='r', label=r'$Burn$' + '-' + r'$in$', lw = 3)\n",
    "    plt.plot(xLim, [np.mean(dx.Accept),np.mean(dx.Accept)], label = '$\\mu_{1/2}$', color='black', lw = 3, ls='--')\n",
    "    plt.xlim(xmin=1,xmax=N)\n",
    "    plt.legend(prop={'size':25});\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/diag_acc.png');      \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pccAll(thin,mm,ix,basePath):\n",
    "    \n",
    "    sig,pcc = cov2cor(thin,mm)\n",
    "    dfSig = pd.DataFrame(sig)\n",
    "    dfPCC = pd.DataFrame(pcc)\n",
    "\n",
    "    # sigma_1\n",
    "    fig, ax1 = plt.subplots(figsize=(32, 24))\n",
    "    dh = dfSig.iloc[:,0].tolist()\n",
    "    nx, xx, px = ax1.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "    kde = sps.gaussian_kde(dh)\n",
    "    yy = kde(xx)\n",
    "    lns1 = ax1.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "    yLim = ax1.get_ylim()\n",
    "    lHDI,rHDI = HDI(dh)\n",
    "    lIQR,rIQR = HDI(dh,0.5)\n",
    "    hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "    hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "    cHDI = np.median(dh)\n",
    "    xCDF,yCDF = ecdf(dh)\n",
    "    yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "    lns2 = ax1.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "    lns3 = ax1.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "    lns4 = ax1.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "    ax1.set_xlabel(r'$\\sigma_1$',  fontsize = 40)\n",
    "    ax1.set_ylabel('Belief',  fontsize = 40)\n",
    "    ax1.set_title(r'$\\sigma_1$ Marginal Posterior | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr,  fontsize = 50)\n",
    "\n",
    "    # second axis\n",
    "    axa = ax1.twinx()\n",
    "    axa.grid(b=None)\n",
    "    lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "    yLimA = axa.get_ylim()\n",
    "    axa.set_ylabel('Probability', fontsize = 40)\n",
    "    axa.set_ylim(bottom=0)\n",
    "    axa.tick_params('y', colors='blue')\n",
    "    lns = lns1+lns5+lns2+lns4+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "\n",
    "    # annotations\n",
    "    ax1.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                 xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                 xytext=(rHDI, 2*yLim[1]/5),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                 xy=(cHDI, yInt), \n",
    "                 xytext=(rHDI, yLimA[1]/2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    ax1.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                 xy=(np.percentile(dh, 60), hIQR), \n",
    "                 xytext=(rHDI, 3*yLim[1]/5),\n",
    "                 textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/sigma_1.png');      \n",
    "    plt.clf()\n",
    "        \n",
    "    # sigma_2\n",
    "    fig, ax2 = plt.subplots(figsize=(32, 24))\n",
    "    dh = dfSig.iloc[:,1].tolist()\n",
    "    nx, xx, px = ax2.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "    kde = sps.gaussian_kde(dh)\n",
    "    yy = kde(xx)\n",
    "    lns1 = ax2.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "    yLim = ax2.get_ylim()\n",
    "    lHDI,rHDI = HDI(dh)\n",
    "    lIQR,rIQR = HDI(dh,0.5)\n",
    "    hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "    hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "    cHDI = np.median(dh)\n",
    "    xCDF,yCDF = ecdf(dh)\n",
    "    yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "    lns2 = ax2.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "    lns3 = ax2.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "    lns4 = ax2.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "    ax2.set_xlabel(r'$\\sigma_2$',  fontsize = 40)\n",
    "    ax2.set_ylabel('Belief',  fontsize = 40)\n",
    "    ax2.set_title(r'$\\sigma_2$ Marginal Posterior | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr,  fontsize = 50)\n",
    "\n",
    "    # second axis\n",
    "    axa = ax2.twinx()\n",
    "    axa.grid(b=None)\n",
    "    lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "    yLimA = axa.get_ylim()\n",
    "    axa.set_ylabel('Probability', fontsize = 40)\n",
    "    axa.set_ylim(bottom=0)\n",
    "    axa.tick_params('y', colors='blue')\n",
    "    lns = lns1+lns5+lns2+lns4+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "\n",
    "    # annotations\n",
    "    ax2.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                 xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                 xytext=(rHDI, 2*yLim[1]/5),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                 xy=(cHDI, yInt), \n",
    "                 xytext=(rHDI, yLimA[1]/2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    ax2.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                 xy=(np.percentile(dh, 60), hIQR), \n",
    "                 xytext=(rHDI, 3*yLim[1]/5),\n",
    "                 textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/sigma_2.png');      \n",
    "    plt.clf()\n",
    "    \n",
    "    # sigma_3\n",
    "    fig, ax3 = plt.subplots(figsize=(32, 24))\n",
    "    dh = dfSig.iloc[:,2].tolist()\n",
    "    nx, xx, px = ax3.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "    kde = sps.gaussian_kde(dh)\n",
    "    yy = kde(xx)\n",
    "    lns1 = ax3.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "    yLim = ax3.get_ylim()\n",
    "    lHDI,rHDI = HDI(dh)\n",
    "    lIQR,rIQR = HDI(dh,0.5)\n",
    "    hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "    hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "    cHDI = np.median(dh)\n",
    "    xCDF,yCDF = ecdf(dh)\n",
    "    yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "    lns2 = ax3.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "    lns3 = ax3.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "    lns4 = ax3.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "    ax3.set_xlabel(r'$\\sigma_3$',  fontsize = 40)\n",
    "    ax3.set_ylabel('Belief',  fontsize = 40)\n",
    "    ax3.set_title(r'$\\sigma_3$ Marginal Posterior | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr,  fontsize = 50)\n",
    "\n",
    "    # second axis\n",
    "    axa = ax3.twinx()\n",
    "    axa.grid(b=None)\n",
    "    lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "    yLimA = axa.get_ylim()\n",
    "    axa.set_ylabel('Probability', fontsize = 40)\n",
    "    axa.set_ylim(bottom=0)\n",
    "    axa.tick_params('y', colors='blue')\n",
    "    lns = lns1+lns5+lns2+lns4+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "\n",
    "    # annotations\n",
    "    ax3.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                 xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                 xytext=(rHDI, 2*yLim[1]/5),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                 xy=(cHDI, yInt), \n",
    "                 xytext=(rHDI, yLimA[1]/2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    ax3.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                 xy=(np.percentile(dh, 60), hIQR), \n",
    "                 xytext=(rHDI, 3*yLim[1]/5),\n",
    "                 textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/sigma_3.png');      \n",
    "    plt.clf()\n",
    "    \n",
    "    # rho_12\n",
    "    fig, ax1 = plt.subplots(figsize=(32, 24))\n",
    "    dh = dfPCC.iloc[:,0].tolist()\n",
    "    nx, xx, px = ax1.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "    kde = sps.gaussian_kde(dh)\n",
    "    yy = kde(xx)\n",
    "    lns1 = ax1.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "    yLim = ax1.get_ylim()\n",
    "    lHDI,rHDI = HDI(dh)\n",
    "    lIQR,rIQR = HDI(dh,0.5)\n",
    "    hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "    hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "    cHDI = np.median(dh)\n",
    "    xCDF,yCDF = ecdf(dh)\n",
    "    yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "    lns2 = ax1.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "    lns3 = ax1.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "    lns4 = ax1.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "    ax1.set_xlabel(r'$\\rho_{12}$',  fontsize = 40)\n",
    "    ax1.set_ylabel('Belief',  fontsize = 40)\n",
    "    ax1.set_title(r'$\\rho_{12}$ Marginal Posterior | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr,  fontsize = 50)\n",
    "\n",
    "    # second axis\n",
    "    axa = ax1.twinx()\n",
    "    axa.grid(b=None)\n",
    "    lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "    yLimA = axa.get_ylim()\n",
    "    axa.set_ylabel('Probability', fontsize = 40)\n",
    "    axa.set_ylim(bottom=0)\n",
    "    axa.tick_params('y', colors='blue')\n",
    "    lns = lns1+lns5+lns2+lns4+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "\n",
    "    # annotations\n",
    "    ax1.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                 xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                 xytext=(rHDI, 2*yLim[1]/5),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                 xy=(cHDI, yInt), \n",
    "                 xytext=(rHDI, yLimA[1]/2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    ax1.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                 xy=(np.percentile(dh, 60), hIQR), \n",
    "                 xytext=(rHDI, 3*yLim[1]/5),\n",
    "                 textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/rho_12.png');      \n",
    "    plt.clf()\n",
    "    \n",
    "    # rho_13\n",
    "    fig, ax2 = plt.subplots(figsize=(32, 24))\n",
    "    dh = dfPCC.iloc[:,1].tolist()\n",
    "    nx, xx, px = ax2.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "    kde = sps.gaussian_kde(dh)\n",
    "    yy = kde(xx)\n",
    "    lns1 = ax2.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "    yLim = ax2.get_ylim()\n",
    "    lHDI,rHDI = HDI(dh)\n",
    "    lIQR,rIQR = HDI(dh,0.5)\n",
    "    hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "    hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "    cHDI = np.median(dh)\n",
    "    xCDF,yCDF = ecdf(dh)\n",
    "    yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "    lns2 = ax2.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "    lns3 = ax2.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "    lns4 = ax2.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "    ax2.set_xlabel(r'$\\rho_{13}$',  fontsize = 40)\n",
    "    ax2.set_ylabel('Belief',  fontsize = 40)\n",
    "    ax2.set_title(r'$\\rho_{13}$ Marginal Posterior | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr,  fontsize = 50)\n",
    "\n",
    "    # second axis\n",
    "    axa = ax2.twinx()\n",
    "    axa.grid(b=None)\n",
    "    lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "    yLimA = axa.get_ylim()\n",
    "    axa.set_ylabel('Probability', fontsize = 40)\n",
    "    axa.set_ylim(bottom=0)\n",
    "    axa.tick_params('y', colors='blue')\n",
    "    lns = lns1+lns5+lns2+lns4+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "\n",
    "    # annotations\n",
    "    ax2.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                 xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                 xytext=(rHDI, 2*yLim[1]/5),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                 xy=(cHDI, yInt), \n",
    "                 xytext=(rHDI, yLimA[1]/2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    ax2.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                 xy=(np.percentile(dh, 60), hIQR), \n",
    "                 xytext=(rHDI, 3*yLim[1]/5),\n",
    "                 textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "\n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/rho_13.png');      \n",
    "    plt.clf()\n",
    "    \n",
    "    # rho_23\n",
    "    fig, ax3 = plt.subplots(figsize=(32, 24))\n",
    "    dh = dfPCC.iloc[:,2].tolist()\n",
    "    nx, xx, px = ax3.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "    kde = sps.gaussian_kde(dh)\n",
    "    yy = kde(xx)\n",
    "    lns1 = ax3.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "    yLim = ax3.get_ylim()\n",
    "    lHDI,rHDI = HDI(dh)\n",
    "    lIQR,rIQR = HDI(dh,0.5)\n",
    "    hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "    hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "    cHDI = np.median(dh)\n",
    "    xCDF,yCDF = ecdf(dh)\n",
    "    yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "    lns2 = ax3.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "    lns3 = ax3.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "    lns4 = ax3.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "    ax3.set_xlabel(r'$\\rho_{23}$',  fontsize = 40)\n",
    "    ax3.set_ylabel('Belief',  fontsize = 40)\n",
    "    ax3.set_title(r'$\\rho_{23}$ Marginal Posterior | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr,  fontsize = 50)\n",
    "\n",
    "    # second axis\n",
    "    axa = ax3.twinx()\n",
    "    axa.grid(b=None)\n",
    "    lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "    yLimA = axa.get_ylim()\n",
    "    axa.set_ylabel('Probability', fontsize = 40)\n",
    "    axa.set_ylim(bottom=0)\n",
    "    axa.tick_params('y', colors='blue')\n",
    "    lns = lns1+lns5+lns2+lns4+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "\n",
    "    # annotations\n",
    "    ax3.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                 xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                 xytext=(rHDI, 2*yLim[1]/5),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                 xy=(cHDI, yInt), \n",
    "                 xytext=(rHDI, yLimA[1]/2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    ax3.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                 xy=(np.percentile(dh, 60), hIQR), \n",
    "                 xytext=(rHDI, 3*yLim[1]/5),\n",
    "                 textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "    \n",
    "    plt.savefig(basePath + '/' + mdl + '/' + ix + '/rho_23.png');      \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Parameter Belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function for parameter marginal posteriors\n",
    "\n",
    "def plot_theta(th,dt,dth,burnVal,N,lags,idx,ix,basePath):\n",
    "    \n",
    "    md = []\n",
    "    mn = []\n",
    "    sd = []\n",
    "    hd = []\n",
    "    hc = []\n",
    "    iq = []\n",
    "    pm = []\n",
    "    \n",
    "    for i in range(th.shape[1]):\n",
    "    \n",
    "        # pdf and cdf distributions\n",
    "        fig, ax1 = plt.subplots(figsize=(32, 24))\n",
    "        dh = th.iloc[:,i]\n",
    "        nx, xx, px = ax1.hist(dh, bins = binFD(dh), normed=1, facecolor='grey', alpha=0.3)\n",
    "        kde = sps.gaussian_kde(dh)\n",
    "        yy = kde(xx)\n",
    "        lns1 = ax1.plot(xx, yy, color = 'grey', label = r'$PDF$')\n",
    "        yLim = ax1.get_ylim()\n",
    "        lHDI,rHDI = HDI(dh)\n",
    "        lIQR,rIQR = HDI(dh,0.5)\n",
    "        hHDI = 0.5*(np.interp(lHDI,xx,yy) + np.interp(rHDI,xx,yy))\n",
    "        hIQR = 0.5*(np.interp(lIQR,xx,yy) + np.interp(rIQR,xx,yy))\n",
    "        cHDI = np.median(dh)\n",
    "        xCDF,yCDF = ecdf(dh)\n",
    "        yInt = np.interp(cHDI,xCDF, yCDF)\n",
    "        lns2 = ax1.plot([lHDI,rHDI], [hHDI, hHDI], color='r', label=r'$HDI_{95\\%}$', lw = 3, ls = '--')\n",
    "        lns3 = ax1.plot([cHDI,cHDI], [yLim[0], np.interp(cHDI,xx,yy)], color='k', label=r'$\\mu_{1/2}$', lw = 3, ls = '--')\n",
    "        lns4 = ax1.plot([lIQR,rIQR], [hIQR, hIQR], color='b', label=r'$IQR$', lw = 3, ls = '--')\n",
    "        ax1.set_xlabel(r'$\\theta_{' + repr(i+1) + '}$',  fontsize = 40)\n",
    "        ax1.set_ylabel('Belief',  fontsize = 40)\n",
    "        ax1.set_title(r'$\\theta_{' + repr(i+1) + '}$ Marginal Posterior | ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + mdl + ' ' + cityStr,  fontsize = 50)\n",
    "        \n",
    "        # second axis\n",
    "        axa = ax1.twinx()\n",
    "        axa.grid(b=None)\n",
    "        lns5 = axa.plot(xCDF,yCDF, label = r'$CDF$', lw = 3, color = 'blue')\n",
    "        yLimA = axa.get_ylim()\n",
    "        axa.set_ylabel('Probability', fontsize = 40)\n",
    "        axa.set_ylim(bottom=0)\n",
    "        axa.tick_params('y', colors='blue')\n",
    "        lns = lns1+lns5+lns2+lns4+lns3\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        axa.legend(lns, labs, loc=2, prop={'size':25})\n",
    "        \n",
    "        # annotations\n",
    "        ax1.annotate(r'$HDI_{95\\%}$ = ' + str(np.around(rHDI-lHDI,decimals=3)), \n",
    "                     xy=((3*rHDI+lHDI)/4, hHDI), \n",
    "                     xytext=(rHDI, 2*yLim[1]/5),\n",
    "                     textcoords='data',\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "        axa.annotate(r'$\\mu_{1/2}$ = ' + str(np.around(cHDI,decimals=3)),\n",
    "                     xy=(cHDI, yInt), \n",
    "                     xytext=(rHDI, yLimA[1]/2),\n",
    "                     textcoords='data',\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "        ax1.annotate(r'$IQR$ = ' + str(np.around(rIQR-lIQR,decimals=3)), \n",
    "                     xy=(np.percentile(dh, 60), hIQR), \n",
    "                     xytext=(rHDI, 3*yLim[1]/5),\n",
    "                     textcoords='data', arrowprops=dict(facecolor='black', shrink=0.05), fontsize=40)\n",
    "        \n",
    "        plt.savefig(basePath + '/' + mdl + '/' + ix + '/post_' + str(i) + '.png');      \n",
    "        plt.clf()\n",
    "        \n",
    "        fig, ax2 = plt.subplots(figsize=(32, 24))\n",
    "        # trace plot\n",
    "        for j in range(nChains):\n",
    "            \n",
    "            x = dt[dt['Chain']==j].iloc[:,3:]\n",
    "            p1 = ax2.plot(np.arange(1,N+1), np.cumsum(x.iloc[:,i])/np.arange(1,N+1),label='Chain ' + str(j), lw = 3)\n",
    "            ax2.scatter(iv, x.iloc[iv-1,i], marker = 'o', label = None, alpha = 0.3, color=p1[0].get_color())\n",
    "            \n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_xlabel('Proposal',  fontsize = 40)\n",
    "        ax2.set_ylabel('Parameter Location',  fontsize = 40)\n",
    "        ax2.set_title(r'$\\theta_{' + repr(i+1) + '}$ MCMC Trace Plot | Chain Length: ' + nStr + ' | ID: ' + mdl + ' ' + cityStr,  fontsize = 50)\n",
    "        xLim = ax2.get_xlim()\n",
    "        yLim = ax2.get_ylim()\n",
    "        ax2.plot([burnVal,burnVal], [yLim[0], yLim[1]], color='r', label=r'$Burn$' + '-' + r'$in$', lw = 3)\n",
    "        ax2.plot(xLim, [np.median(xh.iloc[:,i]),np.median(x.iloc[:,i])], color='grey', lw = 3, ls='--')\n",
    "        ax2.legend(prop={'size':25});\n",
    "        ax2.grid(True)\n",
    "        ax2.set_ylim([yLim[0], yLim[1]])\n",
    "        ax2.set_xlim(left=1,right=N)\n",
    "        plt.savefig(basePath + '/' + mdl + '/' + ix + '/trace_' + str(i) + '.png');     \n",
    "        plt.clf()\n",
    "        \n",
    "        fig, ax3 = plt.subplots(figsize=(32, 24))\n",
    "        # parameter sample autocorrelation\n",
    "        for j in range(nChains):\n",
    "            \n",
    "            x = dth.iloc[idx,:]\n",
    "            x = x[x['Chain']==j].iloc[:,3:]\n",
    "            ax3.plot(np.arange(lags+1),ACF(x.iloc[:,i],lags), label='Thinned Chain ' + str(j), lw = 3, ls = '--')\n",
    "            \n",
    "        ax3.plot(np.arange(lags+1),ACF(dh,lags), color = 'blue', label='Thinned Combined', lw = 3)\n",
    "        ax3.plot(np.arange(lags+1),ACF(xh.iloc[:,i],lags), color = 'red', label='Unthinned Combined', lw = 3)\n",
    "        ax3.legend(prop={'size':25})\n",
    "        ax3.set_title(r'$\\theta_{' + repr(i+1) + '}$ Autocorrelation | ' + mdl + ' ' + cStr + ' Samples (' + fStr + '% Thinning) | ID: ' + cityStr, fontsize=50)\n",
    "        ax3.set_xlabel('Lag', fontsize=40)\n",
    "        ax3.set_ylabel('Normalized Sample Autocorrelation', fontsize=40)\n",
    "        ax3.set_xlim(left=0,right=lags)\n",
    "        plt.savefig(basePath + '/' + mdl + '/' + ix + '/ACF_' + str(i) + '.png');      \n",
    "        plt.clf()\n",
    "        \n",
    "        md.append(cHDI)\n",
    "        mn.append(np.mean(dh))\n",
    "        sd.append(np.std(dh,ddof=1))\n",
    "        hd.append(rHDI-lHDI)\n",
    "        hc.append((rHDI+lHDI)/2)\n",
    "        iq.append(rIQR-lIQR)\n",
    "        pm.append(i)\n",
    "        \n",
    "    return pd.DataFrame({'Median': md,'Mean': mn,'STD': sd,'HDC': hc,'HDI': hd,'IQR': iq,'Theta': pm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle and Combine Parallel MCMC Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable declarations\n",
    "mdlList = ['ARCN','MBCN']\n",
    "nChains = 4\n",
    "mm = 3\n",
    "kk = 4\n",
    "\n",
    "burnVal = int(1e6) # burn-in period\n",
    "warmVal = int(1e6) # number of post-burn-in samples\n",
    "thinVal = int(1e5) # number target samples retained post-burn-in\n",
    "ppdVal = int(1e4)\n",
    "thinFrac = thinVal/warmVal; # fraction retained from warmed samples\n",
    "cVal = int(thinVal*nChains) # total number of thinned samples aggregated parallel chains\n",
    "\n",
    "accT = 0.234\n",
    "lags = 100\n",
    "\n",
    "N = int(warmVal + burnVal)\n",
    "\n",
    "nStr = '%.0e' % N\n",
    "tStr = '%.0e' % thinVal\n",
    "cStr = '%.0e' % cVal\n",
    "fStr = str(int(100*thinFrac))\n",
    " \n",
    "cityNames = pd.read_pickle('Climo.pkl').City.unique()\n",
    "\n",
    "varNames = ['Tmax','Tmin','Vmax']\n",
    "unitNames = ['[K]','[K]','[m/s]']\n",
    "\n",
    "betaList = ['B_0','B_1','B_2','B_3','B_4','B_5','B_6',\n",
    "            'B_7','B_8','B_9','B_10','B_11','B_12',\n",
    "            'B_13','B_14','B_15','B_16','B_17']\n",
    "cityList = ['HAR','GRI','RNO','TVC','SEA','JAN','BIS','BNA','DFW']\n",
    "ARList = ['ARN1', 'ARN2', 'ARN3', 'ARN4', 'ARN5', 'ARN6','ARP1', 'ARP2', 'ARP3', 'ARP4', 'ARP5', 'ARP6']\n",
    "MBList = ['MBN1', 'MBN2', 'MBN3', 'MBN4', 'MBN5', 'MBN6','MBP1', 'MBP2', 'MBP3', 'MBP4', 'MBP5', 'MBP6']\n",
    "SRList = ARList + MBList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j, mdl in enumerate(mdlList):\n",
    "    \n",
    "    if not os.path.exists(mdl):\n",
    "    \n",
    "        os.makedirs(mdl)\n",
    "\n",
    "    dfzs = pd.read_pickle(mdl + '_zscores.pkl')\n",
    "\n",
    "    for i in range(nChains):\n",
    "\n",
    "        dfth_temp = pd.read_pickle(mdl + '_params_' + str(i) + '.pkl')\n",
    "\n",
    "        if i == 0:\n",
    "\n",
    "            dfth = dfth_temp.copy()\n",
    "\n",
    "        else:\n",
    "\n",
    "            dfth = pd.concat((dfth,dfth_temp), ignore_index=True)\n",
    "\n",
    "        del dfth_temp\n",
    "        \n",
    "    dfth['Model'] = mdl  \n",
    "    dfzs['Model'] = mdl\n",
    "    \n",
    "    dfs = dfth[dfth['Proposal'] >= burnVal]\n",
    "    \n",
    "    dfs = dfs.set_index(['Model','Chain','City','Proposal'])\n",
    "    dfth = dfth.set_index(['Model','Chain','City','Proposal'])\n",
    "    dfzs = dfzs.set_index(['Model','Type','Stat','City'])\n",
    "    \n",
    "    dfs = dfs.loc[:,betaList]\n",
    "    \n",
    "    dfs.sort_index(inplace=True)\n",
    "    dfth.sort_index(inplace=True)\n",
    "    dfzs.sort_index(inplace=True)\n",
    "    \n",
    "    dfs.to_pickle(mdl + '_TH.pkl')\n",
    "    dfzs.to_pickle(mdl + '_ZS.pkl')\n",
    "\n",
    "    del dfzs\n",
    "    del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ix = 0\n",
    "\n",
    "    for idx, dx in dfth.groupby(level=['City','Chain']):\n",
    "\n",
    "        dt = dx.drop(['Accept','Width'], axis=1).ix[burnVal:]\n",
    "\n",
    "        # n1 = dt.ix[::10]\n",
    "        # n2 = dt.ix[::100]\n",
    "        # n3 = dt.ix[::1000]\n",
    "        # n4 = dt.ix[::10000]\n",
    "        # n5 = dt.ix[::100000]\n",
    "\n",
    "        n1 = dt.ix[:10]\n",
    "        n2 = dt.ix[:100]\n",
    "        n3 = dt.ix[:1000]\n",
    "        n4 = dt.ix[:10000]\n",
    "        n5 = dt.ix[:100000]\n",
    "\n",
    "        # n1 = dt.ix[npr.choice(np.arange(0,burnVal),burnVal/10,replace=False).astype(int)]\n",
    "        # n2 = dt.ix[npr.choice(np.arange(0,burnVal),burnVal/100,replace=False).astype(int)]\n",
    "        # n3 = dt.ix[npr.choice(np.arange(0,burnVal),burnVal/1000,replace=False).astype(int)]\n",
    "        # n4 = dt.ix[npr.choice(np.arange(0,burnVal),burnVal/10000,replace=False).astype(int)]\n",
    "        # n5 = dt.ix[npr.choice(np.arange(0,burnVal),burnVal/100000,replace=False).astype(int)]\n",
    "\n",
    "        if ix == 0:\n",
    "\n",
    "            N1 = n1.copy()\n",
    "            N2 = n2.copy()\n",
    "            N3 = n3.copy()\n",
    "            N4 = n4.copy()\n",
    "            N5 = n5.copy()\n",
    "\n",
    "        else:\n",
    "\n",
    "            N1 = pd.concat((N1,n1))\n",
    "            N2 = pd.concat((N2,n2))\n",
    "            N3 = pd.concat((N3,n3))\n",
    "            N4 = pd.concat((N4,n4))\n",
    "            N5 = pd.concat((N5,n5))\n",
    "\n",
    "        ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute groups statistics for each level of thinning (i.e., n = 1, 2,..., 5)\n",
    "Mj1 = N1.groupby(level=['City','Chain']).mean()\n",
    "Sj1 = N1.groupby(level=['City','Chain']).var()\n",
    "GR1 = pd.concat([Mj1,Sj1],keys=['Mean', 'Variance'],names=['Stat']).reorder_levels(['City','Stat','Chain'],axis=0)\n",
    "\n",
    "Mj2 = N2.groupby(level=['City','Chain']).mean()\n",
    "Sj2 = N2.groupby(level=['City','Chain']).var()\n",
    "GR2 = pd.concat([Mj2,Sj2],keys=['Mean', 'Variance'],names=['Stat']).reorder_levels(['City','Stat','Chain'],axis=0)\n",
    "\n",
    "Mj3 = N3.groupby(level=['City','Chain']).mean()\n",
    "Sj3 = N3.groupby(level=['City','Chain']).var()\n",
    "GR3 = pd.concat([Mj3,Sj3],keys=['Mean', 'Variance'],names=['Stat']).reorder_levels(['City','Stat','Chain'],axis=0)\n",
    "\n",
    "Mj4 = N4.groupby(level=['City','Chain']).mean()\n",
    "Sj4 = N4.groupby(level=['City','Chain']).var()\n",
    "GR4 = pd.concat([Mj4,Sj4],keys=['Mean', 'Variance'],names=['Stat']).reorder_levels(['City','Stat','Chain'],axis=0)\n",
    "\n",
    "Mj5 = N5.groupby(level=['City','Chain']).mean()\n",
    "Sj5 = N5.groupby(level=['City','Chain']).var()\n",
    "GR5 = pd.concat([Mj5,Sj5],keys=['Mean', 'Variance'],names=['Stat']).reorder_levels(['City','Stat','Chain'],axis=0)\n",
    "\n",
    "GRA = pd.concat([GR1,GR2,GR3,GR4,GR5],keys=[1,2,3,4,5],names=['Order']).reorder_levels(['Order','City','Stat','Chain'],axis=0)\n",
    "\n",
    "# mean within-chain variance\n",
    "W = GRA.xs('Variance',level='Stat').groupby(level=['Order','City']).mean()\n",
    "\n",
    "# between-chain mean of within-chain means\n",
    "ensMean = GRA.xs('Mean',level='Stat').groupby(level=['Order','City']).mean()\n",
    "\n",
    "# between-chain variance of within-chain means\n",
    "B = burnVal*GRA.xs('Mean',level='Stat').groupby(level=['Order','City']).var()\n",
    "\n",
    "# potential scale reduction factor\n",
    "PSRF = ((burnVal - 1)/burnVal + (nChains + 1)*B/(W*nChains*burnVal))**0.5\n",
    "\n",
    "PSRF.columns = list(np.arange(0,18))\n",
    "df = PSRF.reset_index()\n",
    "cols = list(df)\n",
    "cols = cols[2:]\n",
    "dm = pd.melt(df,id_vars=['Order','City'], value_vars=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts = 70\n",
    "fs = 70\n",
    "ls = 35\n",
    "\n",
    "for idx, dx in dm.groupby('City'):\n",
    "    \n",
    "    plt.figure(figsize=(32,24))\n",
    "    pvt = dx.pivot(\"variable\", \"Order\", \"value\")\n",
    "    sns.heatmap(pvt, annot=True, fmt='.1f')\n",
    "    plt.title('Gelman–Rubin Convergence | PSRF | ID: ' + mdl + ' ' + idx, fontsize=ts)\n",
    "    plt.xlabel('Thinning Order: $10^n$ Samples', fontsize=fs)\n",
    "    plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "    plt.xticks(fontsize = ls)\n",
    "    plt.yticks(fontsize = ls)\n",
    "    plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ia = np.arange(1,1e2)\n",
    "ib = np.arange(1e2,1e3,10)\n",
    "ic = np.arange(1e3,N+1,100)\n",
    "iv = np.concatenate((ia,ib,ic), axis=0)\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for ix, dx in dfth.groupby(level=['City']):\n",
    "    \n",
    "    cityStr = ix\n",
    "    \n",
    "    if not os.path.exists(mdl + '/' + ix):\n",
    "        \n",
    "        os.makedirs(mdl + '/' + ix)\n",
    "    \n",
    "    dx = dx.reset_index()\n",
    "    dt = dx.drop(['Accept','Width'], axis=1)\n",
    "    dth = dt[dt['Proposal']>=burnVal].reset_index(drop=True)\n",
    "    idx = npr.choice(np.arange(dth.shape[0]),thinVal*nChains,replace=False).astype(int)\n",
    "    th = dth.iloc[idx,3:]\n",
    "    xh = dth.iloc[:,3:]\n",
    "\n",
    "    # Convergence Diagnostics\n",
    "    plot_diag(dx,burnVal,N,ix,basePath)\n",
    "    \n",
    "    # PCC Plots\n",
    "    pccAll(th,3,ix,basePath)\n",
    "    \n",
    "    # Individual Parameter Plots\n",
    "    tempDF = plot_theta(th,dt,dth,burnVal,N,lags,idx,ix,basePath)\n",
    "    tempDF['City'] = ix\n",
    "    \n",
    "    if ii == 0:\n",
    "        \n",
    "        dfPost = tempDF.copy()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dfPost = pd.concat((dfPost,tempDF), ignore_index=True)\n",
    "        \n",
    "    ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts = 65\n",
    "fs = 65\n",
    "ls = 30\n",
    "\n",
    "plt.figure(figsize=(32,24))\n",
    "pvt = dfPost.pivot(\"Theta\", \"City\", \"Median\")\n",
    "sns.heatmap(pvt, annot=True, fmt='.1f', center=0)\n",
    "plt.title('Posterior Belief | ' + mdl + ' Parameters | Median', fontsize=ts)\n",
    "plt.xlabel('City', fontsize=50)\n",
    "plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "plt.xticks(fontsize = ls)\n",
    "plt.yticks(fontsize = ls)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.figure(figsize=(32,24))\n",
    "pvt = dfPost.pivot(\"Theta\", \"City\", \"Mean\")\n",
    "sns.heatmap(pvt, annot=True, fmt='.1f', center=0)\n",
    "plt.title('Posterior Belief | ' + mdl + ' Parameters | Mean', fontsize=ts)\n",
    "plt.xlabel('City', fontsize=50)\n",
    "plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "plt.xticks(fontsize = ls)\n",
    "plt.yticks(fontsize = ls)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.figure(figsize=(32,24))\n",
    "pvt = dfPost.pivot(\"Theta\", \"City\", \"HDC\")\n",
    "sns.heatmap(pvt, annot=True, fmt='.1f', center=0)\n",
    "plt.title(r'Posterior Belief | ' + mdl + ' Parameters | $HDI_{95\\%}$ Center', fontsize=ts)\n",
    "plt.xlabel('City', fontsize=50)\n",
    "plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "plt.xticks(fontsize = ls)\n",
    "plt.yticks(fontsize = ls)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.figure(figsize=(32,24))\n",
    "pvt = dfPost.pivot(\"Theta\", \"City\", \"STD\")\n",
    "sns.heatmap(pvt, annot=True, fmt='.1f', center=0)\n",
    "plt.title(r'Posterior Belief | ' + mdl + ' Parameters | $\\sigma$', fontsize=ts)\n",
    "plt.xlabel('City', fontsize=50)\n",
    "plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "plt.xticks(fontsize = ls)\n",
    "plt.yticks(fontsize = ls)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.figure(figsize=(32,24))\n",
    "pvt = dfPost.pivot(\"Theta\", \"City\", \"HDI\")\n",
    "sns.heatmap(pvt, annot=True, fmt='.1f', center=0)\n",
    "plt.title(r'Posterior Belief | ' + mdl + ' Parameters | $HDI_{95\\%}$', fontsize=ts)\n",
    "plt.xlabel('City', fontsize=50)\n",
    "plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "plt.xticks(fontsize = ls)\n",
    "plt.yticks(fontsize = ls)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.figure(figsize=(32,24))\n",
    "pvt = dfPost.pivot(\"Theta\", \"City\", \"IQR\")\n",
    "sns.heatmap(pvt, annot=True, fmt='.1f', center=0)\n",
    "plt.title('Posterior Belief | ' + mdl + ' Parameters | IQR', fontsize=ts)\n",
    "plt.xlabel('City', fontsize=50)\n",
    "plt.ylabel(r'Model Parameters: $\\beta_i$',fontsize=fs)\n",
    "plt.xticks(fontsize = ls)\n",
    "plt.yticks(fontsize = ls)\n",
    "plt.gca().invert_yaxis()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
